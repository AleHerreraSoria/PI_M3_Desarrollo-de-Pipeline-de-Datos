# üöÄ Proyecto Integrador: Pipeline de Datos ELT en la Nube

## 1. Introducci√≥n y Contexto del Negocio

Este proyecto implementa un pipeline de datos completo siguiendo el paradigma  **ELT (Extract, Load, Transform)** , dise√±ado para una empresa en expansi√≥n que necesita una soluci√≥n escalable para integrar y analizar grandes vol√∫menes de datos de m√∫ltiples fuentes. El objetivo es transformar datos crudos en conocimiento accionable para optimizar la toma de decisiones estrat√©gicas.

Como prueba de concepto, el pipeline ingesta, procesa, analiza y visualiza un dataset p√∫blico de listados de  **Airbnb en Nueva York** , enriqueci√©ndolo con datos de tipo de cambio obtenidos de una API p√∫blica del  **Banco Central de la Rep√∫blica Argentina (BCRA)** .

## 2. Arquitectura de la Soluci√≥n ‚öôÔ∏è

La arquitectura se construye √≠ntegramente sobre  **Google Cloud Platform (GCP)** , aprovechando sus servicios gestionados para garantizar escalabilidad, rendimiento y mantenibilidad.

### Tech Stack Utilizado

| **Componente**          | **Tecnolog√≠a**       | **Prop√≥sito**                          |
| ----------------------------- | --------------------------- | --------------------------------------------- |
| **Proveedor Cloud**     | Google Cloud Platform (GCP) | Infraestructura escalable y gestionada.       |
| **Data Lake / Staging** | Google Cloud Storage (GCS)  | Almacenamiento de objetos crudos.             |
| **Data Warehouse**      | Google BigQuery             | Almacenamiento y procesamiento de datos.      |
| **Extracci√≥n**         | Python & Docker             | Script para obtener datos de la API del BCRA. |
| **Orquestaci√≥n**       | Apache Airflow              | Automatizaci√≥n y programaci√≥n del pipeline. |
| **CI/CD**               | GitHub & GitHub Actions     | Control de versiones y pruebas automatizadas. |
| **Visualizaci√≥n**      | Jupyter Notebook & Seaborn  | An√°lisis y visualizaci√≥n de resultados.     |

## 3. Estructura del Repositorio

El proyecto est√° organizado de la siguiente manera para separar claramente cada componente:

```
PI_M3_Desarrollo de Pipeline de Datos/
‚îÇ
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ ci.yaml               # Flujo de CI con GitHub Actions
‚îÇ
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ elt_pipeline_dag.py # DAG que orquesta la transformaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yaml       # Configuraci√≥n para levantar Airflow
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ dise√±o_arquitectura.md  # Documento de dise√±o t√©cnico
‚îÇ
‚îú‚îÄ‚îÄ img/
‚îÇ   ‚îî‚îÄ‚îÄ arquitectura_cloud.png  # Diagrama de la arquitectura
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                # Dockerfile para el script de extracci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt          # Dependencias del script de extracci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ script_extract_bcra.py    # Script de extracci√≥n de datos del BCRA
‚îÇ   ‚îú‚îÄ‚îÄ requirements-viz.txt      # Dependencias para el notebook
‚îÇ   ‚îî‚îÄ‚îÄ visualizacion_negocio.ipynb # Notebook de visualizaci√≥n
‚îÇ
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md                   # Este archivo
‚îî‚îÄ‚îÄ requirements-dev.txt        # Dependencias para el entorno de CI

```

## 4. Componentes del Pipeline

### 4.1. Extracci√≥n (Extract) üì•

Un script de Python (`src/script_extract_bcra.py`) se conecta a la API p√∫blica de Estad√≠sticas Cambiarias del BCRA para obtener la √∫ltima cotizaci√≥n del d√≥lar. Este proceso incluye:

* **Validaci√≥n de Calidad:** Verifica que los datos recibidos cumplan con un esquema esperado.
* **Carga en la Nube:** Sube el dato en formato JSON a un bucket en GCS (nuestra capa `raw`).
* **Verificaci√≥n de Carga:** Confirma que el archivo se ha subido correctamente.
* **Contenerizaci√≥n:** El script est√° empaquetado en una imagen de **Docker** (`anhsoria/bcra-extractor:1.0`) publicada en Docker Hub, asegurando su portabilidad.

### 4.2. Carga (Load) ‚û°Ô∏è

Los datos son cargados desde sus fuentes a la capa `raw_data` de nuestro Data Warehouse en  **Google BigQuery** :

* El dataset de Airbnb se carga manualmente desde `AB_NYC.csv`, especificando el esquema y omitiendo el encabezado para garantizar la integridad.
* Los datos del BCRA se cargan desde los archivos JSON en GCS.

### 4.3. Transformaci√≥n (Transform) ‚ú®

Un script de **SQL** se ejecuta dentro de BigQuery para realizar todas las transformaciones. Este proceso crea dos capas adicionales en el DW:

* **Capa `transformed_data` (Plata):** Contiene una tabla (`listings_cleaned`) con los datos limpios, estandarizados y enriquecidos con el tipo de cambio en ARS.
* **Capa `business_layer` (Oro):** Contiene tablas agregadas y optimizadas que responden directamente a las preguntas de negocio.

### 4.4. Orquestaci√≥n üéµ

**Apache Airflow** (desplegado localmente con Docker Compose) act√∫a como el orquestador central del pipeline.

* El DAG `elt_pipeline_dag.py` define el flujo de trabajo.
* Este DAG orquesta la tarea de  **Transformaci√≥n** , ejecutando el script SQL en BigQuery de forma programada (`@daily`) y garantizando la idempotencia mediante el uso de `CREATE OR REPLACE TABLE`. Se opt√≥ por esta soluci√≥n robusta y funcional que cumple con los objetivos del proyecto.

### 4.5. Visualizaci√≥n (Analyze) üìä

Un **Jupyter Notebook** (`src/visualizacion_negocio.ipynb`) se conecta directamente a la capa `business_layer` en BigQuery para:

* Extraer los datos ya procesados y agregados.
* Utilizar librer√≠as como `pandas`, `matplotlib` y `seaborn` para generar visualizaciones claras.
* Presentar las respuestas a las preguntas de negocio de forma gr√°fica e intuitiva.

## 5. CI/CD - Integraci√≥n Continua üîÑ

Se ha configurado un flujo de trabajo con **GitHub Actions** (`.github/workflows/ci.yaml`) que se activa autom√°ticamente con cada `push` o `pull request`. Este flujo de CI realiza una prueba de sintaxis sobre el c√≥digo del DAG de Airflow para prevenir que errores b√°sicos lleguen a producci√≥n.

## 6. C√≥mo Ejecutar el Proyecto

### Prerrequisitos

* Git
* Docker y Docker Compose
* Una cuenta de Google Cloud Platform con un proyecto y facturaci√≥n habilitada.

### Pasos para la Puesta en Marcha

1. **Clonar el Repositorio:**
   ```
   git clone [https://github.com/AleHerreraSoria/PI_M3_Desarrollo-de-Pipeline-de-Datos.git](https://github.com/AleHerreraSoria/PI_M3_Desarrollo-de-Pipeline-de-Datos.git)
   cd PI_M3_Desarrollo-de-Pipeline-de-Datos

   ```
2.  **Configurar Credenciales de GCP:**
-   Crea una cuenta de servicio con los roles: Usuario de BigQuery, Editor de datos de BigQuery y Administrador de objetos de Storage.
-   Descarga el archivo de credenciales JSON, ren√≥mbralo a gcp_credentials.json y col√≥calo dentro de la carpeta src/.

3.  **Cargar Datos Iniciales en BigQuery:**

-   Crea los datasets raw_data, transformed_data y business_layer en tu proyecto de BigQuery.

-   Carga manualmente el archivo AB_NYC.csv en la tabla raw_data.ab_nyc, asegur√°ndote de definir el esquema como STRING y omitir la primera fila de encabezado.

4.  **Ejecutar el Extractor de Datos (una vez):**

-   Navega a la carpeta src/.

-   Ejecuta el contenedor para subir el tipo de cambio a GCS: docker run --rm -v "${pwd}/gcp_credentials.json:/app/gcp_credentials.json" anhsoria/bcra-extractor:1.0 (Aseg√∫rate de tener la imagen bcra-extractor local o usa la versi√≥n p√∫blica).

5.  **Levantar Airflow:**

-   Navega a la carpeta airflow/.

-   Ejecuta: docker compose up -d

-   Accede a la UI en http://localhost:8080 (user: admin, pass: admin).

6.  **Configurar y Ejecutar el DAG:**

-   En la UI de Airflow, ve a Admin > Connections y crea una conexi√≥n google_cloud_default pegando el contenido de tu gcp_credentials.json.

-   Activa y ejecuta manualmente el DAG elt_bigquery_pipeline.

7.  **Visualizar los Resultados:**

-   Navega a la carpeta src/.

-   Instala las dependencias: pip install -r requirements-viz.txt

-   Abre y ejecuta el notebook visualizacion_negocio.ipynb con VS Code o Jupyter.

## 7. Autor
Alejandro Nelson Herrera Soria

LinkedIn: www.linkedin.com/in/alejandro-nelson-herrera-soria

GitHub: https://github.com/AleHerreraSoria
