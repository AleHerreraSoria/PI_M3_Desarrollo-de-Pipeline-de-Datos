# ğŸš€ Proyecto Integrador: Pipeline de Datos ELT en la Nube

## 1. IntroducciÃ³n y Contexto del Negocio

Este proyecto implementa un pipeline de datos completo siguiendo el paradigma  **ELT (Extract, Load, Transform)** , diseÃ±ado para una empresa en expansiÃ³n que necesita una soluciÃ³n escalable para integrar y analizar grandes volÃºmenes de datos de mÃºltiples fuentes. El objetivo es transformar datos crudos en conocimiento accionable para optimizar la toma de decisiones estratÃ©gicas.

Como prueba de concepto, el pipeline ingesta, procesa, analiza y visualiza un dataset pÃºblico de listados de  **Airbnb en Nueva York** , enriqueciÃ©ndolo con datos de tipo de cambio obtenidos de una API pÃºblica del  **Banco Central de la RepÃºblica Argentina (BCRA)** .

## 2. Arquitectura de la SoluciÃ³n âš™ï¸

La arquitectura se construye Ã­ntegramente sobre  **Google Cloud Platform (GCP)** , aprovechando sus servicios gestionados para garantizar escalabilidad, rendimiento y mantenibilidad.

### Tech Stack Utilizado

| **Componente**          | **TecnologÃ­a**       | **PropÃ³sito**                          |
| ----------------------------- | --------------------------- | --------------------------------------------- |
| **Proveedor Cloud**     | Google Cloud Platform (GCP) | Infraestructura escalable y gestionada.       |
| **Data Lake / Staging** | Google Cloud Storage (GCS)  | Almacenamiento de objetos crudos.             |
| **Data Warehouse**      | Google BigQuery             | Almacenamiento y procesamiento de datos.      |
| **ExtracciÃ³n**         | Python & Docker             | Script para obtener datos de la API del BCRA. |
| **OrquestaciÃ³n**       | Apache Airflow              | AutomatizaciÃ³n y programaciÃ³n del pipeline. |
| **CI/CD**               | GitHub & GitHub Actions     | Control de versiones y pruebas automatizadas. |
| **VisualizaciÃ³n**      | Jupyter Notebook & Seaborn  | AnÃ¡lisis y visualizaciÃ³n de resultados.     |

## 3. Estructura del Repositorio

El proyecto estÃ¡ organizado de la siguiente manera para separar claramente cada componente:

```
PI_M3_Desarrollo de Pipeline de Datos/
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yaml               # Flujo de CI con GitHub Actions
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ elt_pipeline_dag.py # DAG que orquesta la transformaciÃ³n
â”‚   â””â”€â”€ docker-compose.yaml       # ConfiguraciÃ³n para levantar Airflow
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ diseÃ±o_arquitectura.md  # Documento de diseÃ±o tÃ©cnico
â”‚
â”œâ”€â”€ img/
â”‚   â””â”€â”€ arquitectura_cloud.png  # Diagrama de la arquitectura
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Dockerfile                # Dockerfile para el script de extracciÃ³n
â”‚   â”œâ”€â”€ requirements.txt          # Dependencias del script de extracciÃ³n
â”‚   â”œâ”€â”€ script_extract_bcra.py    # Script de extracciÃ³n de datos del BCRA
â”‚   â”œâ”€â”€ requirements-viz.txt      # Dependencias para el notebook
â”‚   â””â”€â”€ visualizacion_negocio.ipynb # Notebook de visualizaciÃ³n
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md                   # Este archivo
â””â”€â”€ requirements-dev.txt        # Dependencias para el entorno de CI

```

## 4. Componentes del Pipeline

### 4.1. ExtracciÃ³n (Extract) ğŸ“¥

Un script de Python (`src/script_extract_bcra.py`) se conecta a la API pÃºblica de EstadÃ­sticas Cambiarias del BCRA para obtener la Ãºltima cotizaciÃ³n del dÃ³lar. Este proceso incluye:

* **ValidaciÃ³n de Calidad:** Verifica que los datos recibidos cumplan con un esquema esperado.
* **Carga en la Nube:** Sube el dato en formato JSON a un bucket en GCS (nuestra capa `raw`).
* **VerificaciÃ³n de Carga:** Confirma que el archivo se ha subido correctamente.
* **ContenerizaciÃ³n:** El script estÃ¡ empaquetado en una imagen de **Docker** (`anhsoria/bcra-extractor:1.0`) publicada en Docker Hub, asegurando su portabilidad.

### 4.2. Carga (Load) â¡ï¸

Los datos son cargados desde sus fuentes a la capa `raw_data` de nuestro Data Warehouse en  **Google BigQuery** :

* El dataset de Airbnb se carga manualmente desde `AB_NYC.csv`, especificando el esquema y omitiendo el encabezado para garantizar la integridad.
* Los datos del BCRA se cargan desde los archivos JSON en GCS.

### 4.3. TransformaciÃ³n (Transform) âœ¨

Un script de **SQL** se ejecuta dentro de BigQuery para realizar todas las transformaciones. Este proceso crea dos capas adicionales en el DW:

* **Capa `transformed_data` (Plata):** Contiene una tabla (`listings_cleaned`) con los datos limpios, estandarizados y enriquecidos con el tipo de cambio en ARS.
* **Capa `business_layer` (Oro):** Contiene tablas agregadas y optimizadas que responden directamente a las preguntas de negocio.

### 4.4. OrquestaciÃ³n ğŸµ

**Apache Airflow** (desplegado localmente con Docker Compose) actÃºa como el orquestador central del pipeline.

* El DAG `elt_pipeline_dag.py` define el flujo de trabajo.
* Este DAG orquesta la tarea de  **TransformaciÃ³n** , ejecutando el script SQL en BigQuery de forma programada (`@daily`) y garantizando la idempotencia mediante el uso de `CREATE OR REPLACE TABLE`. Se optÃ³ por esta soluciÃ³n robusta y funcional que cumple con los objetivos del proyecto.

### 4.5. VisualizaciÃ³n (Analyze) ğŸ“Š

Un **Jupyter Notebook** (`src/visualizacion_negocio.ipynb`) se conecta directamente a la capa `business_layer` en BigQuery para:

* Extraer los datos ya procesados y agregados.
* Utilizar librerÃ­as como `pandas`, `matplotlib` y `seaborn` para generar visualizaciones claras.
* Presentar las respuestas a las preguntas de negocio de forma grÃ¡fica e intuitiva.

## 5. CI/CD - IntegraciÃ³n Continua ğŸ”„

Se ha configurado un flujo de trabajo con **GitHub Actions** (`.github/workflows/ci.yaml`) que se activa automÃ¡ticamente con cada `push` o `pull request`. Este flujo de CI realiza una prueba de sintaxis sobre el cÃ³digo del DAG de Airflow para prevenir que errores bÃ¡sicos lleguen a producciÃ³n.

## 6. CÃ³mo Ejecutar el Proyecto

### Prerrequisitos

* Git
* Docker y Docker Compose
* Una cuenta de Google Cloud Platform con un proyecto y facturaciÃ³n habilitada.

### Pasos para la Puesta en Marcha

1. **Clonar el Repositorio:**
   ```
   git clone [https://github.com/AleHerreraSoria/PI_M3_Desarrollo-de-Pipeline-de-Datos.git](https://github.com/AleHerreraSoria/PI_M3_Desarrollo-de-Pipeline-de-Datos.git)
   cd PI_M3_Desarrollo-de-Pipeline-de-Datos

   ```
