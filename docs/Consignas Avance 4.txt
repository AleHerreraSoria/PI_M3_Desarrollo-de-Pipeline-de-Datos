El pipeline de datos ya cuenta con un diseño sólido, extracción automatizada y transformaciones validadas.
Sin embargo, para que el sistema funcione de forma autónoma, confiable y mantenible,
debes avanzar hacia su orquestación y despliegue continuo.
En esta última etapa del proyecto, tu rol como Ingeniero de Datos cobra una dimensión más integral:
debes garantizar la ejecución fluida del pipeline en producción.
Para ello, configurarás Apache Airflow para definir y ejecutar los DAGs que representen todo el proceso ELT
de forma modular y controlada.
Además, implementarás flujos de integración y entrega continua (CI/CD) con GitHub Actions, que aseguren
que cada cambio en el código sea probado automáticamente antes de ser desplegado.
Serás responsable de gestionar dependencias, validar la idempotencia de las tareas y monitorear el sistema.
Esta orquestación automatizada convertirá tu pipeline en un sistema de procesamiento de datos robusto,
listo para escalar junto con el crecimiento de la organización.
Conozcamos las consignas del Avance 4 de nuestro proyecto integrador:

1. Configuración de Apache Airflow mediante Docker Compose y Dockerfile. 
Seguida de la construcción de una imagen contenedorizada lista para ser desplegada en la nube.

2. Definición de DAGs (Directed Acyclic Graphs) en Airflow para orquestar de manera estructurada
todo el proceso de ingesta.  
Transformación y carga de datos.

3. Implementación de una gestión robusta de dependencias entre tareas
Asegurando la idempotencia y un adecuado manejo de errores en el flujo de trabajo.

4. Configuración de flujos CI/CD con GitHub Actions
Para ejecutar automáticamente pruebas en cada push o pull request, asegurando la calidad continua del código.

5. Conocimientos necesarios: Orquestación con Airflow

6. Tech Stack necesario (sugerido, no excluyente):
Airflow
Docker
Python
GiHub Actions
SQL
Snowflake

7. EXTRA CREDIT!
a. Realización de pruebas que verifiquen la ejecución completa del pipeline,
desde la ingesta de datos hasta su almacenamiento final en el Data Warehouse.
b. Ejecución de pruebas que validen el correcto funcionamiento del pipeline,
incluyendo validaciones de integridad, calidad de datos y desempeño del sistema.