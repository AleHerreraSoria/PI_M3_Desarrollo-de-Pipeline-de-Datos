Con el diseño arquitectónico ya aprobado, el siguiente paso para consolidar el proyecto es implementar los mecanismos de recolección de datos. En tu rol como Ingeniero de Datos, debes desarrollar las soluciones que permitirán capturar información proveniente de diversas fuentes: APIs públicas, scraping de sitios web, y otras fuentes definidas en el diseño anterior.

Situación
Tu tarea será crear scripts eficientes y confiables en Python para automatizar la extracción de datos.
Deberás contenerizar este proceso con Docker para facilitar su despliegue en la infraestructura en la nube de la organización. También tendrás la responsabilidad de garantizar que los datos recolectados se almacenen de forma estructurada y coherente en la capa raw, preservando su valor analítico para transformaciones futuras.

El éxito de esta fase sentará las bases para una integración de datos confiable, reutilizable y alineada con los objetivos de negocio.

---------------------------

1. Desarrollo del script de extracción de datos
Implementa un script en Python que obtenga datos desde:
- APIs públicas o privadas (utilizando librerías como requests, httpx o aiohttp).
- Sitios web mediante técnicas de web scraping (con herramientas como BeautifulSoup, Selenium, Scrapy).

Asegúrate de que el script:
- Permita parametrizar endpoints o URLs.
- Tenga manejo de errores y control de fallos.
- Guarde los datos de forma local o en memoria en estructuras como dict, DataFrame, etc.

---------------------------

2. Contenerización del script con Docker
Crea un Dockerfile que:
- Use una imagen base liviana (por ejemplo, python:3.10-slim).
- Instale todas las dependencias necesarias desde requirements.txt.
- Copie el script al contenedor y defina un ENTRYPOINT claro.
- Documenta el Dockerfile con comentarios explicativos.
- Asegúrate de que la imagen se construya correctamente con docker build y funcione con docker run.

---------------------------

3. Integración de datos en la capa Raw
Diseña el proceso para que los datos extraídos desde distintas fuentes (API, scraping, archivos planos, etc.) se integren:
- En un único repositorio de almacenamiento temporal (ej: carpeta raw local, bucket en la nube, base de datos).
- Respetando un esquema común de nombres, formatos y estructura.
- Aplica transformaciones mínimas si es necesario para lograr coherencia entre fuentes. 

---------------------------

4. Verificación de carga en almacenamiento temporal
Implementa una rutina de validación que:
- Verifique que los archivos o registros se almacenan correctamente.
- Asegure la existencia, el tamaño mínimo y la integridad de cada archivo.
- Genere logs o alertas ante errores de carga o duplicaciones.

---------------------------

5. Estandarización del formato de los datos
Convierte y estructura los datos extraídos en formatos adecuados, como:
- CSV para datos tabulares simples.
- JSON para estructuras jerárquicas o semiestructuradas.
- Nombra los archivos según convención clara (por ejemplo: fuente_fecha.json).
- Asegura codificación UTF-8 y delimitadores estándar para evitar errores posteriores.

---------------------------

6. Validación de calidad de los datos
Crea una rutina o script para validar:
- Calidad: valores nulos, tipos correctos, unicidad de claves.
- Completitud: que todos los campos esperados estén presentes.
- Coherencia: que los datos sean consistentes entre sí (por ejemplo, fechas válidas, números positivos).
- Documenta los criterios de validación y genera reportes automáticos.

---------------------------

7. Publicación de la imagen Docker
- Sube la imagen a un entorno de nube (ej: Docker Hub, GitHub Container Registry, Google Artifact Registry).
- Define nombre, versión y tags.
- Asegura que la imagen sea fácilmente accesible y reutilizable por otros miembros del equipo o por el orquestador del pipeline (ej: Airflow, Prefect).
- Documenta cómo utilizarla en la ejecución del pipeline (comandos de docker pull, docker run, etc.).

